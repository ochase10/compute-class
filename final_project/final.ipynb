{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: LLMs\n",
    "\n",
    "### HW1 Problem 2\n",
    "\n",
    "I gave Claude 3.7 Sonnet the exact text of HW1 problem 2 (with the removal of unnecessary details). I have saved its generated code as 'problem1.1_claude.py'. I have made absolutely no edits to the code. On the first prompt, Claude understood the assignment and implemented a well-worked solution. However, it used a rejection-sampling techinuqe for generating velocities and gave a maximum number of iterations before it simply assigned the particle the mean energy. This resulted in almost every particle being given the mean energy. \n",
    "\n",
    "Rather than let that instance run to completion, I reprompted Claude with \"Can you find a way to make the velocity sampling more efficient? It is reaching the maximum attempts for almost every particle. This is likely because the psudo-powerlaw shape means it is extremely unlikely that acceptances will happen at high energies.\" It immediately decided to swap to a numerically integrated CDF technique to generate the samples. This second iteration of the code ran in 234.39 seconds according to its built in timer and saved a plot and data files. Although the code and description of the approach look very good, the resulting plot, which I've renamed 'velocity_dispersion_v2.png' shows that the code does not recover the correct velocity dispersion profile. \n",
    "\n",
    "I reprompted Claude with \"This code is not producing correct results. The sampled result is far greater than the theoretical result in the plot. Make sure you're using Eq. 10 from Hernquist as the theoretical prediction. Also, it seems like you're taking the standard deviation in radial bins to get the velocity dispersion. You should just be taking the mean of the squared radial velocities in each bin instead.\" Claude claimed to make the requested changes along with adding new numerical safeguards and efficieny improvements related to the sampling. It also said it updated the minimum energy at every radius, which if it wasn't doing before would have been a significant bug. I renamed the resulting velocity dispersion plot 'velocity_dispersion_v3.png'. The theoretical curve is now much worse and the match is terrible. The sample might be okay, although it doesn't fall off fast enough at large radii it seems. \n",
    "\n",
    "I prompted it again asking it to fix the theoretical result, use more bins, and to only worry about radii less than 10^4 kpc. The result was actually worse than before. This time the velocity dispersion plot is labeled with v4.\n",
    "\n",
    "I spent a while looking through Claude's code and mine and found that there are issues in the energy range Claude's code samples, and it is back to using the wrong version of Eq 10 from the Herquist paper. I tried to prompt it to fix this, but I exceeded the allowed context window and needed to start a new chat. \n",
    "\n",
    "I made those two changes and the result was much better. The shape of the sample is correct now, but there remains a normalization issue. I'll consider this a near success for Claude. It got the structure of the code correct and was most of the way there, but needed a bit of help to get over the finish line. Unfortunately, that meant I had to go through its code to debug and would still have to do more if I wanted to solve the problem perfectly. \n",
    "\n",
    "The solution it wrote follows the same approach as mine generally, but through very different methods. It compartmentalizes its code similar to me, using separate functions for all the simple equations. Its code is generally tractable, with good commenting. \n",
    "\n",
    "\n",
    "### HW2 Problem 1\n",
    "\n",
    "This time instead of giving Claude the exact problem text, I gave it a more streamlined version: \"Can you write Python code to solve the Kepler problem with a central mass of 1 M_sun,  a=1AU, and e=0.96 using 4th order Runge-Kutta for the integration. Integrate the orbit for 1 year and initialize the particle at perihelion.\"\n",
    "\n",
    "Claude turned out a very sophisticated code that tracks not only the orbit but also the energy and angular momentum conservation. Claude also recognized the need for small timesteps to account for the fast motion of the highly eccentric orbit. It both implemented a small time step and explained why in its accompanying response. It also included the generation of an animation of the orbit in its code, although it commented out the creating of the animation in the main functino. The code it wrote is in 'problem_2.1_claud.py'. \n",
    "\n",
    "I reprompted Claude to have it save the plot and animation as files. The code seems to be an excellent solution to the problem, needing no significant modifications as far as the science is concerned. It had errors in the animation function, but after a couple of adjustments, it manged to find a way that worked. \n",
    "\n",
    "This problem was a massive success for Claude. It solved in on the very first prompt and went above and beyond in its implementation. The solution it wrote is somewhat different than my implementation. It wrote far more code than I did, for one. It seems much more interested in clarity than efficiency, in terms of code length. It created a plot very similar to what I ended up making, which I found very surprising. It couldn't implement the solution all that different and still do it right, but it clearly approaches the coding far differently than me. \n",
    "\n",
    "\n",
    "### Hogg and Foreman-Mackey Problem 2\n",
    "\n",
    "For this exercise, I used ChatGPT's o4-mini model. I decided to try using its \"reasoning\" feature. I gave it the exact text of the problem from the paper. It output a plot of the solution as the question requests. In fact, to get the code, I had to manually choose to see its \"analysis\". The code seems to solve the problem exactly as requested. It is efficient, well-commented, and produces the plot as requested. No modification or reprompting was needed. \n",
    "\n",
    "I tried ChatGPT without the reasoning, and this time it didn't show the solution plot it had generated but rather gave the code and explanation like normal. That code also produces the correct result on the first attempt. This code is slightly less efficient and produces a slightly nicer looking plot. It also chose not to implement its own Gaussian density function.\n",
    "\n",
    "This is a rather simple code, so mine and ChatGPT's versions are very similar. I had written versions that looked like both the standard GPT response and the more efficient reasoning response. There wasn't too much room for flexibility here, but the code it wrote was exceedingly normal. It didn't do anything super fancy or opaque. \n",
    "\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "The LLMs did an overall impressive job solving these problems from the homeworks. The very first problem was by far the most complicated we faced, and the AI struggled with it the most as well. It couldn't replace me for that problem because not only did it requrie multiple re-prompts, it never got to a solution that didn't require debugging from me. \n",
    "\n",
    "For the other 2 problems, they were plenty simple for the AIs to easily turn out sophisticated solutions. If I wanted, I could easily have used AI to do those problems in seconds, and it would have been relatively hard to determine that they were written by LLMs. The code was fairly human, and small edits to syntax and commenting would have made it pretty convincingly mine. Of course, since I could just copy paste the problem text or basically explain it, I didn't need more than the slightest understanding of the problem or the content to get effective solutions. For the MCMC problem, I could have made the requested plot without knowing anything because I literally copy pasted and got the plot back immediately (without even seeing the code in the case of the reasoning model). \n",
    "\n",
    "\n",
    "\n",
    "# Problem 2: Periodic Boundary Conditions\n",
    "\n",
    "In HW3, I wrote the algorithm for calculating the gravitational accelerations on a catalog of particles using a brute force approach. That function did not take advantage of the optimizations of Numpy arrays. Therefore, I rewrote my brute force approach in addition to adding the option for periodic boundary conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_force_acc(cat, ms=None, eta=0, periodic_bc=False, boxsize=None, G=1):\n",
    "    \n",
    "    acc = np.empty(cat.shape)\n",
    "\n",
    "    n = len(cat)\n",
    "    dims = cat.shape[1]\n",
    "\n",
    "    if ms is None:\n",
    "        ms = np.ones(n)[:,None,None]\n",
    "    else:\n",
    "        ms = ms[:,None,None]\n",
    "\n",
    "    if boxsize is None:\n",
    "        boxsize = int(max(cat))+1\n",
    "\n",
    "    coord_dists = np.zeros((n,n,dims))\n",
    "\n",
    "    for i in range(dims):\n",
    "        coord_dists[:,:,i] = cat[:,i:i+1] - cat[:,i:i+1].T\n",
    "        \n",
    "        if periodic_bc:\n",
    "            halfL = boxsize / 2\n",
    "            coord_dists[:,:,i][coord_dists[:,:,i]>halfL] = coord_dists[:,:,i][coord_dists[:,:,i]>halfL] - boxsize\n",
    "            coord_dists[:,:,i][coord_dists[:,:,i]<halfL] = coord_dists[:,:,i][coord_dists[:,:,i]<halfL] + boxsize\n",
    "\n",
    "\n",
    "    denom = np.sum(coord_dists**2, axis=2) + eta**2\n",
    "    nonzeros = (denom > 0)\n",
    "    denom[nonzeros] = denom[nonzeros]**(-1.5)\n",
    "\n",
    "    acc = -G * ms * denom[:, :, None] * coord_dists\n",
    "    acc = np.sum(acc, axis=0)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brute_acc_old(cat, eta=0):\n",
    "    '''\n",
    "    Brute force n-body acceleration function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cat: catalog of object coordinates\n",
    "    f: acceleration function\n",
    "    eta: softening factor\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    nxd array of acceleration on all n particles in all d dimensions\n",
    "    '''\n",
    "    acc = np.zeros(cat.shape)\n",
    "\n",
    "    #double loop to consider force of every particle on every other\n",
    "    for i in range(len(cat)):\n",
    "        for j in range(i):\n",
    "            a = (np.sum((cat[j]-cat[i])**2)+eta**2)**(-1.5)*(cat[j]-cat[i])\n",
    "\n",
    "            #Newton's third law says we don't have to revisit this pairing in reverse\n",
    "            acc[i] -= a\n",
    "            acc[j] += a\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2)\n",
    "dat = rng.random((10000,3))*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = brute_force_acc(dat, boxsize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = brute_acc_old(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(a1-a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reworked my old code slightly so it doesn't use any other functions to make it easier to paste here without clutter. The first test was a sanity check to make sure that my new implementation using fancy Numpy vectorization achieves the same result as the old, naieve algorithm. The output above shows that the results are identical, except that the old algorithm ran for 2 minutes 45 seconds and the new algorithm ran in 7.3 seconds. \n",
    "\n",
    "My new function not only runs faster, but allows for periodic boundary conditions. I implemented the \"minimum image convention\" laid out [here](https://rwexler.github.io/comp-prob-solv/lecture-16-tech-details.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a11 = brute_force_acc(dat, boxsize=1000, periodic_bc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12581\n",
      "[[274.89590071 766.45420008 357.69819844]\n",
      " [ 30.86154458 938.0372885  151.18293521]\n",
      " [168.94833016 396.75096739 675.5774014 ]\n",
      " [162.16598222 611.87997058 539.68288764]]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum((a11-a1)>0))\n",
    "\n",
    "print(dat[abs(np.sum(a11-a1, axis=1)) < 1e-5][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
